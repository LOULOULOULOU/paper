%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第四章
%%==================================================
\chapter{基于深度强化学习的一对一及多对多博弈策略}
\section{引言}
在上一章主要以双人棋盘类博弈策略进行描述，在本章将讨论非棋盘类单智能体对抗和多智能体对抗的问题。对于对称类棋盘游戏来说，其规则简单，并且可落子范围随着双方不断落子逐渐缩小，当棋盘全部走满双方平局，也就是动作双方最多走满全盘步数即可或胜负，应用场景比较单一，对于其他应用场景还有待扩展，除此之外，现有的强化学习模型大多数针对单智能体游戏类的决策任务，对于多智能体强化学习的扩展还有待发掘。原因是当对多智能体进行建模时，每一个智能体不仅需要感知外界环境，同时也需要及时获得其他智能体的信息，其他智能体进行动作可能会影响整个环境状态的变化，因此对于多智能体其动作空间和环境状态空间增大带来训练的困难。根据智能体之间的关系可以分为合作，竞争和既有竞争又有合作，对于这种比较复杂的模型，奖励的分配也是面临的难题之一。本章将介绍多智能体强化学习相关的理论算法基础后，设计一对一智能体相互对抗的实验场景，根据实际情况建立合理的状态动作以及奖励函数，并把实验场景拓展到多对多上，接着展示实验结果并进行分析。
\section{多智能体强化学习理论基础}
在多智能体研究中，智能体之间的关系可以是合作的、竞争的，也可以是两者兼而有之，许多算法都是为智能体特定的关系设立的。
如\cite{Lazaridou2017Multi}主要针对多智能体合作问题提出的，基于假设所有智能体的行为都是在提高集体奖励的基础上进行的。另一种方式是通过共享参数达到合作的目的\ref{Gupta2017Cooperative},这需要所有的智能体模型都相同，奖励也相同。这些算法一般不适用于竞争环境或混合环境。本章将就单智能体间的相互对抗拓展到多智能体间的组队形式的对抗，也就是同队之间智能体是相互协作的，异队间智能体是相互对抗的，所有智能体的优化目标都是降低敌方奖励，增加本方奖励。
下面介绍常用的多智能体强化学习算法。
\subsection{Minimax-Q学习}
Minimax-Q\cite{Littman1994Markov}是一种适用于双人零和博弈的算法。其主要思想是两个智能体之间的奖励函数是互为相反数的，一方利益的最小化同时是另一方利益的最大化，所以两个智能体之间可以共享一个相同的奖励函数，是基于Q-learning算法进行实现的，Minimax—Q学习的值函数是Q值矩阵的最小最大化：
\begin{equation}
{{\rm{V}}^1}{\rm{(s) = }}\mathop {\max }\limits_{{\pi ^1} \in PD({A^1})} \mathop {\min }\limits_{{a^2} \in {A^2}} \sum\limits_{{a^1} \in {A^1}} {{Q_t}^1(s,{a^1},{a^2})} {\pi ^1}
\end{equation}
其中，${\pi ^1}$为第一个智能体的策略，在状态$S$下，当已知第二个智能体采取动作$a_2$,第一个智能体采取动作$a_1$时，其$Q$值函数为:

\begin{equation}
\begin{aligned}
Q_{t + 1}^1(s,{a^1},{a^2}) =& (1 - {\alpha _t})Q_t^1(s,{a^1},{a^2}) + {\alpha _t}[R_t^1(s,{a^1},{a^2}) \\& + \gamma\mathop {\max }\limits_{{\pi ^1} \in PD({A^1})} \mathop {\min }\limits_{{a^2} \in {A^2}} \sum\limits_{{a^1} \in {A^1}} {{Q_t}^1(s,{a^1},{a^2})} {\pi ^1} ]
\end{aligned}
\end{equation}
Minimax-Q学习具有很好的收敛性，但是适用场景相对单一只适用于两人零和博弈，不能应用于多智能体的协作和对抗。
\subsection{Nash-Q学习}
Nash-Q学习改进了Minimax——Q学习中对于零和博弈的局限，主要针对非零和的情况，其认为智能体最后的最优Q函数可以用Nash平衡解定义。假设所有智能体在某一状态$s$下的$Nash$平衡解为${\pi ^1}(s),...,{\pi ^n}(s)$,则第$i$个智能体的值函数定义为：
\begin{equation}
	{V^i}(s) = NashQ_t^i(s) = {\pi ^1}(s)...{\pi ^n}(s)Q_t^i(s)
\end{equation}
所以Nash—Q学习中Q值的更新规则为：
\begin{equation}
Q_{t + 1}^1(s,{a^1},...,{a^n}) = (1 - {\alpha _t})Q_t^1({\rm{s}},{a^1},...,{a^n}) + {\alpha _t}[r_t^1 + \gamma NashQ_t^i(s')]
\end{equation}
当其他智能体进行动作更新时，当前智能体需要不断维护自己的Q值函数。这样计算的复杂度大大增加，同时如何寻求Nash解也是Nash—Q算法面临的困难。
\subsection{Friend-or-Foe Q 学习}
Friend-or-Foe Q 学习把合作和竞争整合到了一个框架下，当对方是Friend时，智能体的个体利益和系统整体利益一致:
\begin{equation}
NashQ_t^i(s) = \mathop {\max }\limits_{{a^1} \in {A^1},{a^2} \in {A^2}} Q_t^1(s,{a^1},{a^2})
\end{equation}
当对方是Foe时，对策变为零和博弈：
\begin{equation}
	NashQ_t^i(s) = \mathop {\max }\limits_{{\pi ^1} \in PD({A^1})} \mathop {\min }\limits_{{a^2} \in {A^2}} \sum\limits_{{a^1} \in {A^1}} {\pi ({a^1})} Q_t^1(s,{a^1},{a^2})
\end{equation}

该算法存在一个问题是需要指定每个智能体和当前智能体的关系，并且要把智能体提前进行划分，每个智能体只具有个体能力不具有全局感知能力。
\section{基于AC框架下的多智能体强化学习}
前面所述经典的多智能体算法都是基于Q值学习进行策略更新的。当其他智能体进行动作时，需要不断更新当前智能体的Q值函数表，同时奖励的分配也是需要加规则进行干预的。这里考虑用AC框架进行多智能体强化学习的研究。奖励不需要人为分配，通过Critic函数告诉Actor当前动作的得分，对于一个有$N$个智能体的博弈环境，假设其策略梯度的参数为$\theta  = \{ {\theta _1},...{\theta _N}\} $，其策略的集合为$\pi  = \{ {\pi _1},...{\pi _N}\} $,对于第$i$个智能体，其期望收益的梯度$J({\theta _i}) = E[{R_i}]$为：
\begin{equation}
	{\nabla _{{\theta _i}}}J({\theta _i}) = {E_{s \sim {p^u},{a_i} \sim {\pi _i}}}[{\nabla _{{\theta _i}}}\log {\pi _i}({a_i}|{o_i}){Q_i}^\pi ({a_i},...{a_N})]
\end{equation}
${\nabla _{{\theta _i}}}J({\theta _i}) = {E_{s \sim {p^u},{a_i} \sim {\pi _i}}}[{\nabla _{{\theta _i}}}\log {\pi _i}({a_i}|{o_i}){Q_i}^\pi ({a_i},...{a_N})]$
其中${Q_i}^\pi ({a_i},...{a_N})$是将所有智能体的动作信息作为输入，对当前智能体的期望收益作为输出的打分函数。对于每一个智能体都有自己的奖励函数结构，因此这个打分函数可以学习到合作和竞争混合的结构。经验池中存储了每一个智能体的观测状态，动作信息和奖励$(x,{a_1},...{a_N},{r_1},...,{r_N})$，
对于每一个智能体，其策略梯度函数的更新方式为：
\begin{equation}
	L({\theta _i}) = {E_{x,a,r,x'}}[{({Q_i}^u(x,{a_1},...,{a_N}) - y)^2}]
\end{equation}

其中$y = {r_i} + \gamma {Q_i}^{u'}(x',{a_1}^\prime ,...,{a'_N}){|_{{{a'}_j} = {{u'}_j}({o_j})}}$
这里$u' = \{ {u_{{{\theta '}_1}}},...,{u_{{{\theta '}_N}}}\} $是当其他智能体进行动作后新的策略。



当利用AC模型进行建模时，对于一对一智能体的网络结构，Actor网络和Critic网络是特征输入平面是相同的，都是把当前棋局作为原始输入，因此可以采用相同的深度学习网络进行特征提取，由于动作策略网络Actor输出的是概率值，Critic网络输出的是评分，在提取特征后可以把网络分成两部分，训练时，Actor根据当前的state选择一个Action，然后Critic可以根据state-action计算一个Q值，作为对Actor动作的反馈。Critic根据估计的Q值和实际的Q值来进行训练，Actor根据Critic的反馈来更新策略。测试时，我们只需要Actor就可以完成，此时不需要Critic的反馈。对于多对多智能体作战而言，对于每个智能体不仅要考虑到敌方智能体的位置，还要考虑我方其他智能体的状态。每个Agent的训练同单个智能体对抗算法的训练过程类似，不同的地方主要体现在Critic的输入上：在单个Agent的算法中，Critic的输入是一个state-action对信息，但是在多对多智能体中，每个Agent的Critic输入除自身的state-action信息外，还可以有额外的信息，比如其他Agent的动作。


\section{一对一单智能体场景设计}
\subsection{问题描述}
首先从单智能体入手，进行问题描述和建模。这里我们设计了一个吃小鱼游戏。
游戏的规则如下：在10*10的网格里，左下角和右上角红蓝两方各有一个智能体，进行相互博弈。红蓝两方交替进行移动，直到一方被另一方吃掉。这里面有两个概念：追击角和逃逸角。红方对蓝方的追击角为红方速度方向和红蓝两方质心连线的夹角。蓝方的逃逸角为蓝方速度方向和蓝方到红方质心连线的夹角。如图\ref{zhuijijiao}所示。
\begin{figure}[!htp]
	\centering
	\includegraphics[width=10cm]{example/zhuijijiao.png}
	\bicaption[这里将出现在插图索引]
	{追击角和逃逸角示意图}
	{Neural network training pipeline}
	\label{fig:zhuijijiao}
\end{figure}
红方吃掉蓝方的规则如下：红方到蓝方距离小于四个单位长度，红方的追击角小于30度，蓝方的逃逸角大于30度。反之蓝方对红方追击角小于30度，红方逃逸角大于30度，蓝方吃掉红方。最后留下的一方获胜。
\subsection{场景建模}
在强化学习框架下，需要对状态空间，动作空间以及奖励函数进行合理表示。对于深度学习部分，需要设计状态空间特征提取的网络结构，以及网络输出的维度和激活函数和损失函数。这里采用蒙特卡洛搜索树进行建模，需要把强化学习思想映射到蒙特卡洛搜索树的结构上，包括根节点和分支包含的信息以及叶子节点返回的值。
首先从游戏规则考虑，游戏获胜的条件和速度方向以及双方的位置信息有关，所以特征平面需要涵盖相关信息，在这里选择5层的10*10的特征空间表示状态信息，第一层为我方当前的位置，第二层为我方上一步的位置，第三层为敌方当前位置，第四层为敌方上一步位置，第五层用来表示我方是红方还是蓝方，红方为全0，蓝方为全1。在动作选择上，由于移动的位置只能是相连的上下左右四个方向，所以动作空间为4个离散的值，0代表向上移动，1代表向下移动，2代表左，3代表右，返回的奖励函数为获胜一方的奖励为1，失败一方奖励为-1。由于小鱼走过的位置还可以重复走，所以这里设置为当两方各自走过的步数超过200步还没有分出胜负，判定为平局，奖励为0。
当每进行一次动作选择，对当前状态进行400次蒙特卡洛树搜索统计搜索结果。根据每个节点的访问次数利用式\ref{eq:gailu}和\ref{eq:zaosheng}进行节点信息统计得到当前状态的动作这里面选取温度常数$\tau=1$,噪声系数$\varepsilon=0.25$。在每一次蒙特卡洛树的搜索和建立中，对于当前位置可选的动作有上下左右，当智能体在围墙边界时，会把不能移动的动作去掉，根据可行动作进行树的节点扩展。对于当前智能体的子节点扩展就是对方可行的动作。在动作选择时，选取最大的$Q+U$进行动作的选择，这里$Q$代表经过当前节点获得的平均奖励，$U$为根据深度学习网络得到的先验概率。

在神经网络参数的选择上面，利用CNN进行特征的提取，网络结构如下：

\section{多对多智能体场景设计}
由于设计的状态包含了当前环境状态和其他智能体的动作信息，在利用深度学习网络拟合值函数和策略函数时，特征提取部分的网络结构可以共享，深度学习网络结构如下图所示：
\begin{figure}[!htp]
	\centering
	\includegraphics[width=10cm]{example/wangluojiegoutu.jpg}
	\bicaption[这里将出现在插图索引]
	{多对多深度学习网络结构图}
	{Neural network training pipeline}
	\label{fig:wangluojiegoutu}
\end{figure}
算法流程图：
\begin{algorithm}[!hbp]
	\caption{多对多强化学习博弈算法}% Ëã·¨±êÌâ
	\begin{algorithmic}[1]%Ò»ÐÐÒ»¸ö±êÐÐºÅ
		\Require
		包含当前位置和历史走过位置的状态信息，游戏动作规则，游戏奖励规则。随机初始化神经网络的参数$\theta_0$
		\Ensure 
		动作策略$p$,状态价值$v$
		\For{每一次迭代$iteration=1$ to $T$ }
		 \If{达到下列之一终止条件：
		 \begin{enumerate}
		     	\item 所有智能体都没有可选动作
		     	\item 达到了游戏最大的行动数还没分出胜负
	     \end{enumerate}}
		\State 跳出当前循环
		\Else
		\State 继续进行本次循环
		\EndIf
		\State 初始化状态$S_0$
		\For {对于在$t$时刻的动作}
		\State 利用$MCTS$算法进行搜索
		\While{$MCTS$搜索次数小于$n_playout$}
		\State 进行树节点的选择：每次模拟通过选择最大上限置信度$Q(s,a)+U(s,a)$进行树的遍历。这里${\rm{U}}(s,a) \propto P(s,a)/(1 + N(s,a))$
		\State 叶节点的扩展和状态值评估：
		\\
		利用神经网络计算结果进行叶节点展开和对应状态值的评估。$(P(s, \cdot ),V(s)) = {f_{{\theta _i}}}(s)$,$p$值的向量存储在从$s$输出的边中。
		\State 回溯：
		\\当进行树的模拟遍历时，被访问的边会进行相应的节点访问次数更新，$Q(s,a) = 1/N(s,a)\sum\nolimits_{s'|s,a \to s'} {V(s')} $,这里${s'|s,a \to s'}$代表在状态$s$采取动作$a$后到达状态$s'$。
		\EndWhile
		\State 动作选择：
		\\当搜索结束后，根据搜索的结果根据式\ref{eq:gailu}结合温度参数统计每个动作被访问的概率。根据得到的结果选择一个动作进行到下一状态$s_t+1$的转移。
		\EndFor
		\State 记录当前幕中经过的所有的状态和最后游戏的结果${r_T} \in \{  - 1, + 1\} $
		\For{在得到最后的获胜结果后，对于之前走过的每一步}
		\State 从当前玩家角度更新最后游戏的得分${z_t} \leftarrow  \pm {r_T}$，结合之前保存的每一步状态，存储数据的格式为$s_t,\pi_t,z_t$
		\EndFor
		\State 对于最新模型self-play产生的数据进行随机采样。
		\State 通过以下损失函数：
		\begin{equation}
		l = {(z - v)^2} - {\pi ^T}\log p + c||{\theta _i}{\rm{|}}{{\rm{|}}^{\rm{2}}}
		\end{equation}
		最大化策略网络输出的概率分布和蒙特卡洛搜索树得到的概率之间的相似度，同时最小化价值函数的预测得分和$self-play$得到的最后结果之间的误差。
		调整输入给蒙特卡洛树的神经网络参数$(p,v) = {f_{{\theta _i}}}(s)$。
		\If {$iteration\%check_point==0$}
		\State 利用最新数据训练出来的模型(参数为$\theta '$)和之前最好的模型(参数为$\theta$)进行对弈
		\If {获胜率大于0.8}
		 \State 进行网络参数更新${\theta _{i + 1}} = \theta '$，用于下一次$self-play$数据生成中的先验概率。
		\Else
		 \State 不进行网络参数更新，${\theta _{{\rm{i + 1}}}}{\rm{ = }}\theta $
		\EndIf
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

从上述算法可以看出，$MCTS$可以看作是一个策略改进的实现，$MCTS$在深度学习输出的先验概率${f_{{\theta _{i - 1}}}}$基础上搜索策略${\pi _t} = {\alpha _{{\theta _{i - 1}}}}({s_t})$。树的边建立了一系列状态动作对$(s,a)$，记录了先验概率$P(s,a)$,节点访问次数$N(s,a)$和状态动作值$Q(s,a)$。$self-play$的过程可以看作策略迭代网络的标签数据，在每一次自我博弈中，选择获胜玩家的数据进行一个正样本。训练深度学习网络权重${\theta _i}$,价值网络损失函数为回归常用的均方误差${(z - v)^2}$，策略网络为交叉熵$ - {\pi ^T}\log p$。这里同时训练两个网络所以将两项目标函数进行加和，利用梯度下降法进行网络参数的更新。


\section{实验设计与实验结果}
this is a test
\subsection{一对一实验结果}
\subsection{多对多实验结果}
\section{本章小结}


