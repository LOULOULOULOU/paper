%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第四章
%%==================================================
\chapter{基于深度强化学习的棋类博弈策略}
计算机博弈是人工智能领域面临的挑战之一，由博弈模型发展出的算法在很多领域都有成功的应用。本章将详细介绍博弈理论的基本知识，并结合AlphaZero原理实现五子棋模型以及对探索利用平衡问题和深度学习速率问题进行改善。

\section{引言}
作为人工智能的重要载体，机器博弈已经渗透到人类社会生活的各个领域，如利用博弈理论做用户数据分析\cite{吴诚2017基于博弈论的大用户直购电双边决策研究}，网络防御系统的搭建\cite{许晓燕2018基于博弈模型的网络防御},电力需求侧分析\cite{刘晓峰2018博弈论在电力需求侧的应用研究综述}，房产土地竞标\cite{朱传军2011基于模糊测度与模糊积分的房地产评估方法与应用},等等。

常见的博弈模型分为完备信息的博弈模型，和非完备信息的博弈模型。完备信息是指博弈双方都已知当前的局面状态并能感知历史信息，非完备信息博弈是指博弈双方无法感知状态信息或者只能感知部分状态信息进行相互博弈。根据博弈双方最后取胜目标又可以分为零和博弈和非零和博弈。零和博弈是指博弈的双方最后获取的总奖励和为零，当一方获胜的同时必然会导致另一方的失败。零和博弈是一种非合作式的博弈模型，与之相对的是非零和博弈，即博弈的双方奖励不会因为对手奖励增加而变少。根据玩家采取动作的时效性可以分为范式博弈(Form Game)和扩展式博弈(Extensive Form Game),范式博弈是指玩家双发同时做出动作，或即使有动作有先后顺序在本回合结束之前看不到对方当前的动作，扩展式博弈是指玩家双方交替轮流进行动作，后手玩家能够感知先手玩家所做的动作。在这里，我们主要讨论完备信息下的两人零和扩展式博弈模型。典型的完备信息零和棋盘类博弈模型有象棋，五子棋，围棋，等等。AlphaGo的问世更是将深度强化学习在零和博弈模型的应用推到了巅峰，AlphaGo 的算法结合了蒙特卡洛搜索树，深度学习网络，强化学习Actor-Critic框架。下面将详细介绍算法的技术背景，整个算法框架的组成，本文结合实际应用进行的改进以及实验仿真结果。
\section{AlphaGo和AlphaZero技术背景}

围棋博弈面临的最大挑战就是其庞大的搜索空间带来的高计算复杂度和特征平面难以完全建立。传统的棋类博弈有利用完全遍历的蒙特卡洛树搜索的方法和利用监督学习训练模型的方法。然而对于一些状态空间过于庞大的系统来说，完全遍历所有可能是难以实现的，大量人工棋谱获取数据成本很高，监督信号是最后一步的胜负，中间的过程很难被监督模型学习到。对于这种具有时间延迟的监督信号，研究者开始把方案转换到了强化学习上。自2016年1月谷歌DeepMind团队提出AlphaGo成果以来，2016年3月与世界围棋冠军李世石的对弈中以4:1取得胜利，2016年，该算法在中国棋类网站注册账号和数十位中日韩围棋高手对决连续60局无一败局，接着在2017年5月的中国乌镇举办的围棋峰会上，与世界第一围棋冠军柯洁对战，以3：0获胜，等级高于世界最高水平。其成功的背后离不开以下几个基本算法的应用：基于监督学习网络的预训练，蒙特卡洛搜索树的策略和价值标签的产生，深度学习网络强大的拟合能力，以及基于强化学习框架的self-play形式的数据扩充。
\subsection{蒙特卡洛树}
双人零和博弈过程可以用简单描述为对于互为对手的黑方和白方，奖励分别为$R^1$和$R^2$，$R^1+R^2=0$ ，当黑方获胜时黑方的奖励最大化为$R^*$，白方的奖励最小化为$-R^*$。结合强化学习思想，双方期望优化的奖励如式\ref{eq:minmax}。
\begin{equation}
\label{eq:minmax}
{v_*}(s) = \mathop {\max }\limits_{{\pi ^1}} \mathop {\min }\limits_{{\pi ^2}} {v_\pi }(s)
\end{equation}
其中，${v_\pi }(s) = {E_\pi }[{G_t}|{S_t} = s]$。
基于完全信息的博弈问题都可以通过建立树模型来模拟对弈过程。在棋类游戏中一个回合的对弈过程可以通过一颗树完整的表示出来，博弈树的结构如图\ref{fig:tree}。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\hsize]{example/searchtree.jpg}
	\bicaption[博弈树示意图]
	{博弈树示意图}
	{Schematic diagram of game tree.}
	\label{fig:tree}
\end{figure}

在博弈树中根节点代表了起始状态信息，从一个节点到其子节点的过程称为动作，节点的子节点数称为分支因子，博弈树的末端节点是博弈无法继续进行的节点，携带了游戏结束的奖励信息。博弈树是一种递归的数据结构，每进行一次行动后，根节点的子节点又作为下一状态信息的根节点进行下一步搜索工作。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\hsize]{example/minimaxtree.jpg}
	\bicaption[极小极大搜索树结构示意图]
	{极小极大搜索树结构示意图}
	{Minimax search tree structure diagram.}
	\label{minimaxsearchtree}
\end{figure}
选择动作的方案最简单的想法就是建立一颗极小极大搜索树，其示意图如图\ref{minimaxsearchtree}。极小极大树提出的核心是在损失尽量小的前提下增加本方的收益，图中，在第一步黑方是极大方，在动作选择的过程中会选择$a_1$获得极大收益-2，对于白方来说是极小方，在第二层动作选择时会选择第二层收益为-2 的子节点$b_2$，图中红色的线代表当前状态的搜索路径。这样一直循环进行选择直到游戏结束，就可以知道当前棋局下所有动作获胜的概率，越接近最后一步奖励的动作获得的累计奖励越大。但是对于搜索空间比较大的情况，遍历所有动作是难以实现的，这就需要建立更加高效的搜索树。考虑到深度网络有强大的拟合能力，可以用神经网络拟合最后的输出结果代替真实结果$v(s,w) \approx {v_*}(s)$，这样能提高搜索树的效率。此外由于存在大量低质量的子树分支，如何在优良分支进行更多扩展，砍掉不需要的子树分支也是需要考虑的另一个问题。
为了解决搜索树过于庞大的问题，蒙特卡洛树应运而生，蒙特卡洛树的思想是从当前给定状态开始，随机采样对后续棋局进行模拟，经过多次采样将结果的平均值返回该节点作为评估获胜率的基准。一般来说，节点的数据结构包括以下三个部分${Q,N,W}$,其中，$Q(v)$ 为经过该节点s所获得的总评分，$N$为该节点及其子节点被访问的次数，$W$为总的奖励收益。一个完整的蒙特卡洛树包括四个步骤如图\ref{fig:treesearch}。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\hsize]{example/treesearch.jpg}
	\bicaption[蒙特卡洛搜索树流程]
	{蒙特卡洛搜索树流程}
	{Process of monte carlo search tree.}
	\label{fig:treesearch}
\end{figure}

\begin{enumerate}
	\item 选择(Selection):对于已经建立好的搜索树从根节点向下扩展到子节点选择路径的方案。
	\item 扩展(Expansion):对于没有建立好的树节点部分，在这个叶节点下建立新的叶节点。
	\item 模拟(Simulation):从叶节点开始，随机的进行一系列动作直到结束，进而评估其选择动作的好坏。
	\item 回溯(Backpropagation):利用最终奖励值，把结果返回给从$C$到根节点的连边信息：$Q \leftarrow \frac{W}{N},W \leftarrow W + R,N \leftarrow N + 1$
\end{enumerate}

下面分别介绍这几个步骤的具体实现过程。在选择过程，通常利用到UCT函数，UCT算法相对于传统随机模拟过程减少了前期大量的尝试探索过程，通过迭代的方式构建子树，能够在每一步进行选择的时候考虑到之前所获取的先验函数信息。在蒙特卡罗树搜索树遍历过程中，遵循节点UCT最大化。UCT函数的定义为式\ref{eq:uct}。
\begin{equation}
\label{eq:uct}
UCT({v_i},v) = \frac{{Q({v_i})}}{{N({v_i})}} + c\sqrt {\frac{{\log (N(v))}}{{N(vi)}}} 
\end{equation}

在这里，$v$代表当前节点，$ v_i$ 代表其子节点，UCT算法一定程度解决了强化学习算法中最大的问题，即探索和利用的平衡问题。公式的左边是利用部分(exploitation component)，右边的探索部分(exploration component)有效的避免了动作选择陷入局部最优解，让树能够有效的遍历更多未被访问或者访问次数很少的节点，为其提高更大的概率。一个节点的信息由奖励和访问次数构成，具有较高平均奖励的节点鼓励访问，具有较低访问次数的节点同时鼓励被开发。这里$c$是控制探索和利用平衡关系的参数，在模拟过程为了得到最后的奖励结果，当访问到之前没有访问过的节点时，需要进行模拟过程得到预计的奖励，即rollout函数。扩展过程中由rollout选中的策略并不认为是已展开的节点，只有当搜索到达当前节点，并真实进行动作才认为当前节点被展开。模拟相当于是一个随机过程，通常用服从均匀分布的随机采样得到。AlphaGo结合了深度学习模型和强化学习模型完善了蒙特卡洛搜索树的搜索过程。

\subsection{AlphaGo算法原理}
AlphaGo成功的把监督学习网络和强化学习原理结合在一起，其主要思路是利用专家数据作为先验知识指导强化学习中的动作选择和评估。AlphaGo由两种网络共四个神经网络系统组成：利用专家数据的监督学习网络和用自我博弈方式产生数据的强化学习网络。如图\ref{fig:net}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\hsize]{example/net.pdf}
	\bicaption[深度学习网络组成]
	{深度学习网络组成}
	{Deep learning network composition}
	\label{fig:net}
\end{figure}

首先利用人类专家数据进行训练如下两个网络：基于快速走子策略的${\rho _\pi }$网络，即$roll out$网络，这是一个浅层的快速网络，用来给蒙特卡洛搜索树的模拟阶段提供评估信息，相当于是一个多分类的问题。其输入是当前局面信息，输出是各个位置的落子概率。基于监督学习的深层网络${\rho _\sigma }$更加深层，拟合能力更强，相对速度更慢，其为后面强化学习策略网络初始化参数信息，同时为蒙特卡洛搜索树中在动作选择时提供先验的动作概率。
接着基于强化学习原理，建立两个网络，一个是基于策略的${\rho _\rho }$网络，和历史经验池中的网络进行相互博弈，最大化自己的奖励输出。一个是基于状态价值的${v_\theta }$网络，其根据前面博弈数据利用回归的原理进行建立的。在这里，策略网络和价值网络输入都是原始的棋盘信息，策略网络输出的是每个可以落子位置的概率，概率越大代表当前选手得分越高，这个网络也相当于是一个多分类网络，通过不断和自己经验池中的其他网络进行自我博弈来提升效果。价值网络输出是当前状态的评分，相当于回归网络。这两个网络分开进行训练如图\ref{fig:policyandvalue}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{example/policyandvalue.pdf}
	\bicaption[策略网络和价值网络结构]
	{策略网络和价值网络结构}
	{Neural network training pipeline.}
	\label{fig:policyandvalue}
\end{figure}

\subsection{AlphaZero算法原理}

AlphaZero的出现巅峰了传统机器学习依赖大量数据的需求。根据白板理论，用专家数据学习到的结果并不一定都是可靠的，因此AlphaZero认为数据的产生不一定需要依赖人类专家数据，事实证明，这个理论效果好的超出预期：在只有8个小时的训练时间里击败了AlphaGo,又在4个小时的训练后击败了顶级国际象棋引擎Stockfish，再经过2个小时的训练后击败了日本Elmo引擎。不利用人类数据却得到了比专家方案更好的解决方案，人工智能又上了一步台阶。AlphaZero相比于AlphaGo做了如下改进：
\begin{enumerate}
	\item 在特征提取方式上，AlphaZero不再使用经过人工处理的特征，而选择原始的棋盘信息作为系统的输入，通过深度学习网络真正实现了端到端式学习。
	\item 在蒙特卡洛树快速走子策略中，摒弃随机走子策略和专家数据训练网络的方式，而直接使用强化学习策略梯度网络进行快速走子。提高了蒙特卡洛树预估结果的准确性。
	\item 在网络结构层面，简化了网络形式，主体网络只有策略网络和价值网络。这两个网络前期特征提取工作大同小异，所以对网络进行了合并操作，大大降低了网络的复杂度。这样以轻微增加策略网络的预测错误率为代价大大降低了价值网络预测的误差。
\end{enumerate}

AlphaZero对UCTS进行了改进，每次动作选择最大化上限置信区间，如式\ref{eq:uct1}。
\begin{equation}
\label{eq:uct1}
{\rm{Q(}}\mathop s\limits^ \to  {\rm{,}}\mathop a\limits^ \to  {\rm{) + U(}}\mathop s\limits^ \to  {\rm{,}}\mathop a\limits^ \to  {\rm{)}}
\end{equation}

其中，${\rm{U(}}\mathop s\limits^ \to  {\rm{,}}\mathop a\limits^ \to  {\rm{)}} \propto \frac{{P(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )}}{{1 + N(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )}}$。Q是定义的叶子节点的值，将会在下面提到。

叶子节点扩展的方式由基于监督学习网络得到的先验概率改为依据强化学习网络端策略网络输出的策略概率。每个叶子节点存储了以下信息：先验概率$P(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )$,访问次数$N(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )$,行动价值$Q(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )$。
在模拟过程，每遍历一条边更新统计数据，访问次数${N(\mathop s\limits^ \to  ,\mathop a\limits^ \to  )} +=1$,更新行动价值：$Q(\vec{s},\vec{a})=\frac{1}{N(\vec{s},\vec{a})}\sum_{\vec{s}'\vert \vec{s},\vec{a}\Rightarrow \vec{s}'}V(\vec{s}')$, 其中$\vec{s}'\vert \vec{s},\vec{a} \Rightarrow \vec{s}'$表示模拟过程中从$\vec{s}$走到$\vec{s}'$的所有落子行动$\vec{a}$。
在回溯的阶段，
根据UCT算法向下遍历叶子节点，直到得到最后的结果，进行回溯操作。
在网络结构上，使用一分为二的形式，即前期特征提取部分使用相同的网络结构，因为策略网络输出的是多分类问题，所以最后一层用全连接和softmax函数，损失函数为交叉熵。而价值网络是回归问题，最后一层用tanh函数，损失函数为均方误差损失函数。拟合的数据就是前期蒙特卡洛搜索树搜索得到的结果。
把蒙特卡洛树和深度学习网络组合在一起的示意图如图\ref{fig:Alphazero1}。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\hsize]{example/AlphaZero1.pdf}
	\bicaption[AlphaZero模型示意图]
	{AlphaZero模型示意图}
	{Schematic diagram of AlphaZero model.}
	\label{fig:Alphazero1}
\end{figure}

通过蒙特卡洛树得到每个状态下的$({s_t},{\pi _t},{z_t})$,其中$s_t$表示$t$时刻的状态信息，$\pi_t$表示$t$时刻蒙特卡洛搜索树的动作分布，定义为式\ref{eq:fenbu}。

\begin{equation}
\label{eq:fenbu}
{\pi _{\rm{t}}}(a|{s_t}) = \frac{{N({s_{\rm{t}}},a)}}{{\sum\nolimits_b {N({s_t},b)} }}
\end{equation}

最后神经网络拟合的损失函数如式\ref{eq:loss}。
\begin{equation}
\label{eq:loss}
l = {(z - v)^2} - {\pi ^T}\log p + c{\rm{||}}\theta {\rm{|}}{{\rm{|}}^2}
\end{equation}

其中$v$和$p$分别是策略价值网络输出的评分值和动作概率值。

\section{一种改进的AC强化学习算法}

正如经典算法提到，强化学习面临的主要问题是探索和利用的平衡问题，当探索能力越强，对应的搜索树越宽，搜素速率较慢，利用能力越强，树的深度越深，最后结果可能会陷入局部最优解。此外自我对弈数据的不稳定导致神经网络拟合难度加大。针对以上问题本章提出了两点改进:
\begin{enumerate}
\item 引入随机噪声，鼓励选择更多没有访问的节点，避免陷入局部最优,在强化学习框架中平衡探索和利用关系时，同时引入$\epsilon$贪心算法和softmax算法，促进前期探索后期利用的学习过程。
\item 在神经网络的拟合部分结合参数的相对熵用到自适应学习率的方法，加快网络前期的训练过程，在越接近最优解时缩小学习率。适应数据前期不稳定，后期接近标签奖励明确的特点。
\end{enumerate}



\subsection{基于相对熵的自适应学习率}
在原来的AlphaZero论文算法中，学习率的算法是根据迭代次数进行调整，寻求最优解的方式为异步随机梯度下降法，初始化的学习率为$\alpha$,当迭代次数为$T$时递减一半，为$2T$时再次递减，直到寻得最优解，这样学习率$\alpha$以及时间$T$都需要人为进行选取，当参数选取不合适很容易导致梯度不稳定过拟合或欠拟合现象的出现。而且迭代次数并不一定能直接反应出网络梯度更新效果的好坏，调参负担重导致结果容易出现崩溃的现象。这里我们选取能够直接反应网络更新前后变化量大小及效果相对熵作为衡量是否需要调整步长的指标。相对熵的定义如下：
\begin{equation}
{\rm{D}}(P{\rm{||Q}}){\rm{ = }}\sum\limits_{x \in X} {p(x)\log \frac{{P(x)}}{{Q(x)}}} 
\end{equation}

根据相对熵的变化自适应调整学习率，当相对熵变化大于一定阈值后适当减小学习率，反之，当相对熵变化小于一定的阈值加大学习步长。结合强化学习网络，定义前后差距如式\ref{eq:dist}。

\begin{equation}
\label{eq:dist}
dist =  - \sum\limits_{i = 1}^{|A|} {{p_{\bar \theta }}(s)(\log ({p_\theta }(s)) - \log ({p_{\bar \theta }}(s))} )
\end{equation}

这里, $p_{\bar \theta}$代表状态更新前的网络参数，$p_\theta $代表状态更新后的网络参数，相对熵代表了网络更新前后的变化程度，因此可以根据相对熵如式\ref{eq:r}自适应调整学习率。

\begin{equation}
\label{eq:r}
r = \left\{ {\begin{array}{*{20}{c}}
	{r/\lambda ,dist > threshold\_up}\\
	{r,others}\\
	{r*\lambda ,dist < threshold\_down}
	\end{array}} \right.
\end{equation}

其中，$\lambda$代表每次更新学习率放缩的比例，$threshold\_up$和$threshold\_down$分别代表了上限阈值和下限阈值。

\subsection{引导式探索}

为了进一步解决强化学习探索和利用的平衡问题，式\ref{eq:fenbu}已经介绍了经过蒙特卡洛搜索树搜索后统计的状态动作概率值，考虑到动作选择的概率很大程度依靠于蒙特卡洛树多次模拟得到的统计结果，在网络训练初期，很多时候动作选择是随机性的，对神经网络的拟合借鉴性不大，而在网络后期，越接近奖励的步骤目标越明确，这时候可以增大对应的借鉴比例。softmax策略是根据Boltzmann分布引入温度常量$\tau $,动作概率公式变为：

\begin{equation}
\label{eq:gailu}
{\pi _t}(a|{s_t}) = \frac{{{e^{N{{({s_t},a)}^{1/\tau }}}}}}{{{e^{\sum\nolimits_b {N{{({s_t},b)}^{^{1/\tau }}}} }}}}
\end{equation}
在这里${N({s_t},a)}$是在状态$s_t$时选择动作$a$的次数，${\sum\nolimits_b {N{{({s_t},b)}^{1/\tau }}} }$是在状态$s_t$时，选择所有动作的总次数，这里面温度常数$1>\tau>0$,是控制探索和利用平衡的指标，$\tau$越小，代表动作策略更倾向于选择访问次数多的动作,当$\tau$趋近于零时函数趋近于仅利用，$\tau$越大，动作概率趋近于选择更多未探索到的策略，$\tau$趋近于正无穷，策略趋近于仅探索。这里可以在开局把$\tau$设为1，鼓励智能体探索不同情况的动作，在训练的后期，令$\tau  \to 0$，当训练策略稳定后，智能体将选择访问次数较多的动作。
为了进一步增加探索的几率，对动作概率引入随机噪声如式\ref{eq:zaosheng}。
\begin{equation}
\label{eq:zaosheng}
	P({s_t},a) = \varepsilon{\pi _t}(a|{s_t}) + (1-\varepsilon) {\eta _a}
\end{equation}
其中，${\pi _t}(a|{s_t})$即为\ref{eq:gailu}中的动作概率值，${\eta _a} \sim {\rm{Dir}}(c)$中$c$为狄利克雷噪声(Dirichlet noise)中的常数，$\varepsilon$为平衡噪声和概率值的常数。
所以改进后的自适应学习率调整的算法流程如算法\ref{algo:new}。
\begin{algorithm}[htpb]
	\caption{基于相对熵的自适应学习率引导式强化学习算法}% Ëã·¨±
	\label{algo:new}
	\begin{algorithmic}[1]
		\Require ~~ \\
		初始学习率$r_0$,上限阈值$threshold\_up$和下限阈值$threshold\_down$，调整率$\lambda$,网络初始参数$\theta_0$，以及温度常数$\tau$和狄利克雷常数$c$,平衡系数$\varepsilon $。
		\While {没有达到网络优化停止准则}
		\State 从经验池中采样m个样本${\rm{\{ }}{{\rm{x}}^{(1)}}{\rm{,}}{{\rm{x}}^{(2)}}{\rm{,}}...{\rm{,}}{{\rm{x}}^{(m)}}{\rm{\} }}$
		\State 根据式\ref{eq:gailu}统计在$t$时刻状态$s_t$下选择不同动作的概率。
		\State 根据\ref{eq:zaosheng}更新概率值。
		\State 当利用神经网络进行概率值拟合时，根据式\ref{eq:dist}计算网络参数更新前后相对变化量。
		\State 然后利用式\ref{eq:r}更新学习率。进行下一轮蒙特卡洛树的更新和神经网络参数的更新
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\section{针对五子棋双人博弈场景模型性能分析}

本节主要针对上一节提出的两点改进算法进行模型参数性能的分析，根据分析曲线和具体的应用场景进行参数的选择。由于研究的是博弈式一对一智能体策略，因此以五子棋为实验背景进行多组对比实验。

\subsection{数据及场景介绍}
五子棋是大家耳熟能详的简单棋类游戏，其游戏规则比较简单，在一个$n \times n$的棋盘里，黑手和白手交替进行一次落子，可落子的空间为全盘，所以在搜索树的第一层有$n \times n$种动作可行区域，第二层有$n \times n-1$种动作，依次递减。当玩家首先在棋盘中在横向纵向以及对角上连成连续的五个棋子即判定为获胜。这里为了简化搜索空间，棋盘设为$8 \times 8$大小。

抽象成强化学习模型，为了表示黑白双方的位置信息，考虑五子棋落子位置和对手上一步落子位置有很大关系，这里用4层的$8 \times 8$的棋盘表示落子信息。第一层为当前玩家全部落子位置，第二层表示对方玩家全部落子位置，第三层为对手最后一步落子的位置。由于零和博弈是极小极大化的过程所以需要最后一层平面表示当前玩家是黑方还是白方，黑方为全1的矩阵，白方为全0的矩阵。在动作方式的建立上，棋盘可落子范围为全盘，所以初始化动作为$8 \times 8$的概率矩阵，当某一位置已落子，将该位置从动作池中去掉。五子棋是有严格胜负规则的游戏，奖励可以根据游戏的胜负规则来确定：进行到游戏的最后如果当前玩家获胜，奖励为+1，对手奖励为-1，反之如果当前玩家失败，奖励为-1，对手为+1，如果棋盘全满，双方无子可落，双方平局，奖励全部为0。

在利用深度学习网络进行前期特征提取时，所有实验都用4层卷积网络进行特征的提取工作，在输出中，策略网络看成多分类问题，激活函数选用softmax函数，价值网络看成回归问题，输出函数用tanh函数。

每下一步用400次蒙特卡洛仿真，经过self-play形式进行博弈数据的生成，把数据放进经验池中，五子棋棋盘信息是严格对称的，而博弈数据的产生又是比较缓慢的，所以这里采用对期棋盘数据分别进行90度，180度，270度的翻转操作，这样有效进行了数据扩充，弥补了随机采样带来的样本利用率低的缺点。

\subsection{参数对模型效果影响曲线}
在这一节对比了不同噪声系数以及学习率调整系数对实验结果的影响。可以通过计算策略网络的信息熵反应策略网络决策的分布离散程度，信息熵越小代表策略越明确，信息熵越大代表决策过程越趋向于随机。同时监控了神经网络的损失函数随迭代次数变化曲线，当损失函数呈下降趋势说明网络是趋近于收敛的，下降的越平滑网络越稳定。策略网络损失代表神经网络动作策略网络和蒙特卡洛树统计概率的偏差，价值网络代表神经网络和获胜结果之间回归的偏差，网络优化的总损失函数为策略损失和价值损失之和。这里具体把两个损失函数分开，看不同参数得到的网络模型效果对比。

首先对比不同噪声系数对各种参数的影响，如图\ref{fig:noise}所示。图\ref{fig:noise:a}展示了不同噪声系数下策略网络的交叉熵，图\ref{fig:noise:b}展示了损失函数随迭代次数的改变，损失函数包括策略网络损失函数和价值网络损失函数，损失函数越小代表网络拟合self-play数据效果越好。图\ref{fig:noise:c}描绘了策略损失函数曲线。图\ref{fig:noise:d}描绘了价值函数损失曲线。
一个好的网络应该信息熵随着迭代次数增长呈下降趋势，当$\varepsilon$接近1时，完全依赖softmax策略进行动作选择，信息熵基本不变，损失函数曲线没有下降，网络并没有学到合适的策略。当$\varepsilon$为0.5时，以一半的概率随机选取动作，此时模型收敛效果不好，信息熵以及损失函数都没有下降。当$\varepsilon$在0.6到0.8之间相对曲线下降比较平缓，策略损失函数也呈下降趋势，价值损失函数波动不定，这和搜索过程中只有最后一步才会有明确标签，数据不稳定有关。

\begin{figure}[htpb]
	\centering
	\subcaptionbox{噪声系数对信息熵的影响\label{fig:noise:a}}
	{\includegraphics[width=0.45\hsize,height=0.45\hsize]{example/epsilon-entropy1.jpg}}
	\hspace{0.5em}
	\subcaptionbox{噪声系数对损失函数的影响\label{fig:noise:b}}
	{\includegraphics[width=0.45\hsize,height=0.45\hsize]{example/epsilon-loss1.jpg}}
	\newline
	\centering
	\subcaptionbox{噪声系数对策略损失函数的影响\label{fig:noise:c}}
	{\includegraphics[width=0.45\hsize,height=0.45\hsize]{example/epsilon-policyloss1.jpg}}
	\hspace{0.5em}
	\subcaptionbox{噪声系数对价值损失函数的影响\label{fig:noise:d}}
	{\includegraphics[width=0.45\hsize,height=0.45\hsize]{example/epsilon-valueloss1.jpg}}
	\bicaption
	{噪声系数对不同参数的影响}
	{The influence of noise coefficient on different parameters.}
	\label{fig:noise}
\end{figure}

\begin{figure}[hbpt]
	\centering
	\subcaptionbox{学习率调整系数对信息熵的影响\label{fig2response:a}}
	{\includegraphics[width=0.45\hsize,height=0.4\hsize]{example/lambda-entropy1.jpg}}
	\subcaptionbox{学习率调整系数对损失函数的影响\label{fig2response:b}}
	{\includegraphics[width=0.45\hsize,height=0.4\hsize]{example/lambda-loss1.jpg}}
	\newline
	\centering
	\subcaptionbox{学习率调整系数对策略损失函数的影响\label{fig2response:c}}
	{\includegraphics[width=0.45\hsize,height=0.4\hsize]{example/lambda-policy1.jpg}}
	\subcaptionbox{学习率调整系数对价值损失函数的影响\label{fig2response:d}}
	{\includegraphics[width=0.45\hsize,height=0.4\hsize]{example/lambda-value1.jpg}}
	\bicaption
	{学习率调整系数对不同参数的影响}
	{The influence of adjustment coefficient of learning rate on different parameters.}
	\label{fig2response}
\end{figure}

图\ref{fig2response}展示了不同学习率调整系数对信息熵，损失函数的影响。当调整步长过大时，曲线基本没有下降趋势，减小调整步长，损失函数开始逐渐下降，信息熵开始平缓下降，但是过小的调整步长会导致收敛变慢，所以选取合适的调整系数能够更有利于网络模型的收敛。
\section{基于五子棋的单智体博弈实验}
在上一节参数选择经验的基础上，本节实验参数设置如表\ref{canshu}所示。

\begin{table}[htpb]
	\centering
	\bicaption[五子棋实验参数设置]
	{五子棋实验参数设置}
	{Gomoku experiment parameter setting.}
	\label{canshu}
	\begin{tabular}{ll} \toprule
		参数名称   & 参数值  \\  \midrule
		棋盘大小 & 8*8 \\ 
		自适应学习率参数 & 1.0 \\ 
		温度探索参数$\tau$& 1.0 \\ 
		每次落子蒙特卡洛搜索次数 & 400 \\ 
		批训练样本大小 & 500 \\ 
		迭代次数 & 1500 \\ 
		\bottomrule
	\end{tabular}
\end{table}


经过1500次训练模型基本收敛。下面分析训练好的模型。在初始位置，神经网络对各个位置落子的概率如表\ref{tab:gailv}。
\begin{table}[htbp]
	\centering
	\bicaption[深度学习网络对初始位置落子概率的预测]
	{深度学习网络对初始位置落子概率的预测}
	{Prediction of initial position drop probability by deep learning network.}
	\label{tab:gailv}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline 
		1.475e-4 & 1.060e-4 & 5.279e-5 & 1.107e-4 & 9.022e-5 & 4.170e-5 & 1.109e-4 & 1.193e-4 \\ 
		\hline 
		1.584e-4 & 6.658e-5 & 1.091e-3 & 5.847e-4 & 7.405e-4 & 8.547e-4 & 7.423e-5 & 8.784e-5 \\ 
		\hline 
		3.247e-5 & 7.867e-4& 4.690e-2 & 8.819e-3 & 7.429e-3&4.045e-2 & 8.091e-4 & 3.269e-5 \\ 
		\hline 
		9.957e-5& 5.741e-4 & 7.397e-3 & 0.184 & 0.198 & 0.011& 5.795e-4 & 7.367e-5 \\ 
		\hline 
		6.310e-5 &6.247e-4& 9.647e-3& 0.168 & 0.178 & 1.057e-2 & 7.635e-4 & 9.389e-5 \\ 
		\hline 
		3.174e-5 & 7.434e-4 & 4.978e-2 & 1.201e-2 & 1.064e-2 & 4.221e-2 & 6.999e-4 & 3.441e-5 \\ 
		\hline 
		1.170e-4 & 5.157e-5& 8.608e-4 & 7.298e-4 & 5.024e-4 & 9.605e-4 & 4.176e-05 & 8.261e-5\\ 
		\hline 
		1.148e-4 &7.760e-5 &  5.001e-5 & 1.053e-4 & 1.379e-4 & 2.526e-05 & 6.797e-05 & 1.110e-4 \\ 
		\hline 
	\end{tabular} 
\end{table}

可以看出，模型倾向于把初始落子位置放在中间，这也符合正常规则的五子棋的经验：第一步越靠近中间越有利。一定程度说明了网络学习到参数的有效性。图\ref{figAIvsAI}展示了模型自我博弈过程。其中，棋子上的整数标号记录了落子顺序，浮点数代表模型对当前局面下的动作的打分。

\begin{figure}[htpb]
	\centering
	\includegraphics[width=6cm]{example/AVA1.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA2.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA3.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA4.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA5.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA6.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA7.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVA8.png}
	\bicaption[智能体自我博弈过程图示]
	{智能体自我博弈过程图示}
	{Diagram of agent's self-game process.}
	\label{figAIvsAI}
\end{figure}

接下来就人机对弈过程中模型的具体输出做详细分析,人类执白手，AI模型执黑手。在初始位置经过400步搜索模型得到的统计概率为表\ref{tab:moxinggailv2}。
\begin{table}[htbp]
	\centering
	\bicaption[初始位置搜索树统计概率]
	{初始位置搜索树统计概率}
	{Statistical probability of search tree at initial position.}
	\label{tab:moxinggailv2}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
	\hline 
	0& 0 & 0 & 0 & 0 & 0 &  0&  0\\ 
	\hline 
	0& 0 & 0 & 0 & 0 & 0 &  0&  0\\ 
	\hline 
	0 & 0 & 0.0125 & 0.005 & 0.0015 & 0.0125 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0.1925 & 0.3425 & 0.02 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0.0015 & 0.205 & 0.245 & 0.01 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0.01 & 0.0025 & 0.005 & 0.015 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  
	\hline 
\end{tabular} 
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=6cm]{example/AVH1.png}
	\hspace{0.5cm}
	\bicaption[五子棋人机博弈初始位置]
	{五子棋人机博弈初始位置}
	{Gomoku man-machine game initial position}
	\label{figAIvsHuman}
\end{figure}

初始局面如图\ref{fig:AIvsHuman}所示。接下来的人机对弈过程如图\ref{fig:AIvsHuman}所示。计算机每次根据搜索树得到的最大概率进行动作的选择。

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=6cm]{example/AVH2.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVH3.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVH4.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVH5.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVH6.png}
	\hspace{0.5cm}
	\includegraphics[width=6cm]{example/AVH7.png}
	\bicaption[五子棋人机对弈过程图示(1).]
	{五子棋人机对弈过程图示(1)}
	{Diagram of man-machine game of Gomoku(1).}
	\label{fig:AIvsHuman}
\end{figure}


经过几轮对战后到达图\ref{fig:human4:a}所示状态。搜索树得到的统计概率为表\ref{tab:moxinggailv3}。选择概率最大的进行动作，到达如图\ref{fig:human4:b}所示状态。这时策略神经网络输出概率值信息为表\ref{tab:celueshenjingwangluoshuchu}。对照蒙特卡洛搜索得到的结果，深度学习网络输出的结果基本和搜索统计结果一致。观察当前状态对于黑方叶子节点预测奖励为0.97883，根据贪心策略选择概率最大的动作执行，最后到达图\ref{fig:human6}状态。


\begin{figure}[hbpt]
	\centering
	\subcaptionbox{\label{fig:human4:a}}
	{\includegraphics[width=6cm]{example/AVH8.png}}
	\hspace{0.5em}
	\subcaptionbox{\label{fig:human4:b}}
	{\includegraphics[width=6cm]{example/AVH9.png}}
	\bicaption[五子棋人机对弈过程图示(2).]
	{五子棋人机对弈过程图示(2).}
	{Diagram of man-machine game of Gomoku(2).}
	\label{fig:human4}
\end{figure}

\begin{table}[htbp]
	\centering
	\bicaption[搜索树统计概率]
	{搜索树统计概率}
	{Statistical probability of search tree.}
	\label{tab:moxinggailv3}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
	\hline 
	0& 0 & 0 &0  & 0 &0 0.2425 & 0 &0  \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0.615 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0& 0 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0.135 & 0 & 0.0025 & 0 & 0 & 0.0025 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
	\hline 
\end{tabular}
\end{table}


\begin{table}[htbp]
	\centering
	\bicaption[策略神经网络输出]
	{策略神经网络输出}
	{Strategy neural network output.}
	\label{tab:celueshenjingwangluoshuchu}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline 
		4.364e-3 & 1.047e-3 & 7.525e-3 & 2.883e-3 & 6.946e-4 & 5.545e-3 & 0.057 & 0.309 \\ 
		\hline 
		3.495e-3 & 1.008e-2 & 9.739e-3 & 7.828e-05 & 1.271e-3 & 3.174e-05 & 7.719e-4 & 7.758e-4 \\ 
		\hline 
		5.235e-4 & 7.740e-3 & 6.412e-4 & 5.572e-05 & 1.263e-06 & 1.740e-4 & 3.681e-3 & 6.366e-4 \\ 
		\hline 
		7.379e-2 & 1.574e-2 & 6.703e-07 & 6.327e-06 & 3.206e-05 & 1.603e-05 & 5.863e-2 & 9.232e-3 \\ 
		\hline 
		2.077e-4 & 1.737e-05 & 1.134e-4 & 2.158e-05 & 2.920e-06 & 1.697e-4 & 5.049e-4 & 2.913e-2 \\ 
		\hline 
		9.056e-2 & 1.897e-2 & 3.469e-06 & 2.165e-3 & 2.638e-3 & 1.505e-3 & 1.165e-2 & 0.128 \\ 
		\hline 
		8.939e-4 & 2.178e-4 & 4.440e-4 & 8.316e-2 & 1.967e-3 & 8.452e-4 & 5.612e-4 & 4.942e-4 \\ 
		\hline 
		9.371e-4 & 6.839e-4 & 4.050e-3 & 3.668e-4 & 2.882e-2 & 3.848e-4 & 6.134e-4 &5.001e-4 \\ 
		\hline 
	\end{tabular} 
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=6cm]{example/AVH10.png}
	\hspace{0.5cm}
	\bicaption[五子棋人机对弈过程图示(3)]
	{五子棋人机对弈过程图示(3)}
	{Diagram of man-machine game of Gomoku(3).}
	\label{fig:human6}
\end{figure}


最后黑方获胜，通过实验可知距离奖励越近策略越明确，在本实验中，观察策略网络输出的交叉熵可以看出智能体在训练过程中随着迭代的进行策略越接近于明确，损失函数不断下降。在人机博弈时，神经网络基本能准确输出各个位置获胜的概率，和蒙特卡洛树统计得到的概率基本一致。在游戏结束时，奖励越来越明确，智能体也能准确感知当前的状态信息并作出决策。在自我博弈过程中，连个智能体相互对抗直到和棋，说明训练出的智能体基本棋力相当，也证明了用自我博弈方式产生数据的有效性。由于棋盘过小，硬件资源有限，这里只给出了$8 \times 8$的情况，又由于强化学习训练比较困难，稳定性不高，在本章只以五子棋为例介绍了一对一智能体零和博弈相关实验，对于其他应用场景的拓展还有待开发。

\section{本章小结}

本章主要针对双人棋盘博弈模型进行探索和研究。在文章的第一部分介绍了博弈模型的概念和应用，第二部分介绍了AlphaGo和AlphaZero的基本原理，AlphaGo利用了人类监督数据进行网络的初始化处理以及搜索树的快速走子，AlphaZero则从无到有完全利用自我博弈数据进行深度学习网络的拟合。针对于强化学习探索和利用的平衡问题以及在深度学习网络更新中学习率难以选择的问题，在本章的第三部分提出了两点改进：受多臂老虎机模型的启发，这里用温度系数控制动作选择部分对探索和利用的平衡，避免因探索权重大带来的计算复杂度高和利用权重大带来模型陷入局部最优解的问题，为了进一步避免动作过于单一，在概率分布上加入随机噪声；用相对熵衡量策略网络输出参数的相对变化范围，根据网络参数的变化自适应调整学习率，在网络训练前期加快步长，在接近最优解时减小步长，既能提高训练效率又能避免局部最优。在文章的第四部分针对改进的方案结合五子棋游戏模型讨论了在不同噪声系数和不同学习率调整参数下网络损失函数和信息熵的变化曲线，根据输出曲线合理选择参数。在第五部分针对改进算法设计了五子棋实验，展示了人机对战结果和智能体之间自博弈结果。针对人机博弈过程详细展示了蒙特卡洛搜索树搜索结果和神经网络输出概率值，在结果上验证了改进算法的有效性。