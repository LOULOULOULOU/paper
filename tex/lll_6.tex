%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第四章
%%==================================================
\chapter{总结与展望}
\section{总结}
本文主要研究深度强化学习在零和博弈模型上的应用。尽管深度强化学习在一些场景上已经取得一定成就，但是应用场景的局限使其在很多方向还有很多探索的空间。博弈模型和强化学习思想结合案例更是很少。本文针对回归数据的扩充以及飞机一对一和二对二模型的具体应用场景，改进了深度强化学习模型，为深度强化学习和零和博弈模型的结合提供了新思路。
具体而言，本文的研究内容和创新点总结如下：
\begin{enumerate}
	\item  针对回归数据的扩充问题，本文提出了有监督信号的WGAN网络应用于回归数据的扩充问题上。针对WGAN网络生成样本覆盖不均的问题，这里使用自动编码机对原始数据进行编码然后使用WGAN作为解码器，使WGAN能够覆盖所有样本分布。在提出算法改进方案后，使用标准数据集对模型的性能进行了验证，并分析了自编码器在不同权重下模型的表现。最后针对电子设备参数以及股票数据的预测问题，使用改进算法进行实验分析。
	\item 针对棋盘类一对一智能体零和博弈问题，本文提出了如下两个创新点:一是基于相对熵自适应学习率的方法，根据深度学习网络参数的相对熵调整学习步长加快收敛速度。而是针对强化学习探索与利用平衡的问题把$\varepsilon$贪心策略和$softmax$策略进行结合，引入随机噪声避免模型陷入局部最优解。
	\item 在一对一智能体对弈模型的基础上拓展为二对二智能体对弈，将棋盘类模型拓展到飞机对抗问题上，首先建立了飞机一对一对抗模型，分析模型人机对弈的过程，证明场景拓展的可行性，进一步将一对一拓展为二对二，利用强化学习模型自动分配队间和对内奖励，展示实验结果并分析了结果的合理性。
\end{enumerate}

综上所述，本文主要从数据的产生过程，强化学习指导棋盘类博弈策略以及指导空战类场景二对二作战策略等方面进行了研究和尝试。用三个核心章节从不同方面探索了深度强化学习模型与零和博弈理论结合的理论和应用分析。每章都根据应用场景进行相应的模型改进，性能分析以及实验结果展示。为深度强化学习的应用拓展了新思路。

\section{研究展望}

作为新兴热门的研究方向，深度强化学习的发展和应用现在仍停留在初级的尝试和应用阶段，可延展和改进的地方还有很多。对于以后深度强化学习研究工作的展望，有以下几点：
\begin{enumerate}
	\item 在利用GAN生成数据的研究上，本文主要探讨的是连续型回归数据的扩充，对于离散型数据的梯度以及收敛性问题仍有待探讨。
	\item 对于一对一智能体博弈问题，本文把场景局限在了零和完美信息博弈的场景，有明确的的胜负规则情况奖励相对容易定义，对于一些非完美信息，如何对其他智能体建模以及奖励函数的建立需要依靠更多的实验以及理论研究。
	\item 对于多智能体对抗的问题本文研究的是当智能体数量比较少情况下的策略问题，面临智能体数量不对等情况下的博弈问题，如何利用树模型进行建模以及奖励的回溯方式。当面临大量智能体时，庞大的状态空间也是对工程上的挑战。
\end{enumerate}

总之，现有强化学习的成功应用都是基于游戏场景，对其他场景的拓展，以及针对应用场景相应理论的改进和创新将成为未来深度强化学习研究的热点。