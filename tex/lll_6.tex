%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第四章
%%==================================================
\chapter{总结与展望}
\section{总结}
本文主要研究深度强化学习在小样本数据和无先验数据进行决策。尽管深度强化学习在一些场景上已经取得一定成就，数据的获取模型的建立以及奖励函数的分配都限制了其在不同场景的应用。本文针对回归数据的扩充以及飞机一对一和二对二模型的具体应用场景，利用博弈模型和强化学习思想结合产生数据，并针对具体问题提出了相应的改进措施，给强化学习在数据扩充，空战决策问题上提供了新的思路。{\tiny }具体而言，本文的研究内容和创新点总结如下：
\begin{enumerate}
	\item  针对数值类回归数据的扩充问题，本文提出了一种新的有监督信号的WGAN算法。针对WGAN网络生成样本覆盖不均的问题，这里使用自动编码机对原始数据进行编码然后使用WGAN作为解码器，使WGAN能够覆盖所有样本分布。在提出算法改进方案后，使用标准数据集对模型的性能进行了验证，并分析了在不同参数下模型的表现。最后针对电子设备参数以及股票数据的预测问题，使用改进算法和传统数据扩充算法进行实验对比分析。
	\item 针对棋盘类一对一智能体零和博弈问题，本文提出了如下两个创新点:一是基于相对熵自适应学习率的方法，根据深度学习网络参数的相对熵调整学习步长加快收敛速度。而是针对强化学习探索与利用平衡的问题把$\varepsilon$贪心策略和$softmax$策略进行结合，引入随机噪声避免模型陷入局部最优解。
	\item 将棋盘类模型拓展到飞机对抗问题上，首先建立了飞机一对一对抗模型，分析模型人机对弈的过程，证明场景拓展的可行性，进一步将一对一拓展为二对二，利用强化学习模型自动分配队间和队内奖励，展示实验结果并分析了结果的合理性。
\end{enumerate}

综上所述，本文主要从数据的产生过程，强化学习指导棋盘类博弈策略以及指导空战类场景二对二作战策略等方面进行了研究和尝试。用三个核心章节从不同方面探索了深度强化学习模型与零和博弈理论结合的理论和应用分析。每章实验都根据应用场景进行相应的模型算法改进，性能分析以及实验结果展示。为深度强化学习的应用拓展了新思路。

\section{研究展望}

作为新兴热门的研究方向，深度强化学习的发展和应用现在仍停留在初级的尝试和应用阶段，可延展和改进的地方还有很多。对于以后深度强化学习研究工作的展望，有以下几点：
\begin{enumerate}
	\item 在利用GAN生成数据的研究上，由于生成网络和判别网络损失函数是互为相反的会存在震荡问题，如何加快网络收敛速度仍有待探讨。
	\item 对于一对一智能体博弈问题，本文把场景局限在了零和完美信息博弈的场景，有明确的的胜负规则情况奖励相对容易定义，对于一些非完美信息，如何对其他智能体建模以及奖励函数的建立需要依靠更多的实验以及理论研究。
	\item 对于多智能体对抗的问题本文研究的是当智能体数量比较少情况下的策略问题，面对二对一智能体不对等博弈问题以及更多更复杂智能体对抗问题，是否依旧采用树模型进行建模以及奖励的回溯方式还需考量。当面临大量智能体时，庞大的状态空间也是对工程上的挑战。
\end{enumerate}

总之，现有强化学习的成功应用都是基于规则已知的游戏场景，对其他场景的拓展，以及针对不同应用场景相应理论的改进和创新将成为未来深度强化学习研究的热点。