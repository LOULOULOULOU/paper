%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第四章
%%==================================================
\chapter{总结与展望}
\section{总结}
本文主要研究深度强化学习在零和博弈模型上的应用。尽管深度强化学习在一些场景上已经取得一定成就，但是应用场景的局限使其在很多方向还有很多探索的空间。博弈模型和强化学习思想结合案例更是很少。本文针对回归数据的扩充以及飞机一对一和二对二模型的具体应用场景，改进了深度强化学习模型，为深度强化学习和零和博弈模型的结合提供了新思路。
具体而言，本文的研究内容和创新点总结如下：
\begin{enumerate}
	\item  针对回归数据的扩充问题，本文提出了有监督信号的WGAN网络应用于回归数据的扩充问题上。针对WGAN网络生成样本覆盖不均的问题，这里使用自动编码机对原始数据进行编码然后使用WGAN作为解码器，使WGAN能够覆盖所有样本分布。在提出算法改进方案后，使用标准数据集对模型的性能进行了验证，并分析了自编码器在不同权重下模型的表现。最后针对电子设备参数以及股票数据的预测问题，使用改进算法进行实验分析。
	\item 针对棋盘类一对一智能体零和博弈问题，本文提出了如下两个创新点:一是基于相对熵自适应
\end{enumerate}
在现有研究成果的基础上实现了基于监督信号的改进的WGAN网络对数据的扩充，基于相对熵改进学习率的引导式强化学习算法在五子棋一对一博弈模型上的应用，扩展到二对二既有合作又有竞争情况下的深度强化学习的应用。本全文共包括了以下几部分：
在本文的第一章介绍了深度强化学习的研究背景，包括发展历史，发展现状，同时也提出了现有研究的不足：WGAN网络应用于图像数据场景比较多，对于回归预测问题没有有效的应用案例，同时模式崩溃导致调参困难。强化学习的探索与利用以及模型的收敛性很难保证，同时和博弈模型进行结合应用不广泛，利用深度强化学习在无数据情况下做多对多智能体博弈的成果也十分稀少。
在文章的第二部分介绍了强化学习基本的理论知识，然后延伸到深度强化学习，介绍了常用的深度强化学习算法，对比了使用场景和优缺点。为后续的章节奠定了理论基础。
在第三部分利用