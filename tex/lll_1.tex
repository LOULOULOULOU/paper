%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% chapter01.tex for SJTU Master Thesis
%%第一章
%%==================================================
\chapter{绪论}
\section{课题背景及研究意义}
\subsection{课题研究背景}
人工智能(Artificial Intelligence,AI)是研究，开发用于模拟延伸和扩展人的认知，识别，分析和决策的多种功能的一门新兴科学技术，同时涵盖了统计学，计算机科学，脑神经科学和社会科学等学科。人工智能的应用领域包括机器人，语音识别，图像识别，自然语言处理和专家系统等等。从人工智能被首次提出到现在还不到一个世纪的时间里，由于科技发展水平的阻力已经经历了几次大起大落：1956年在达特茅斯会上将不同研究领域的学者组织在一起，首次提出了人工智能的概念，标志着人工智能成为一个独立的研究领域。美国国防部高级研究计划局拨款220万美元给MIT用于人工智能研究工作带来了AI的第一次高潮时期，在1970年前后，由于数据量不够加上算法实现能力不足，和基础研究理论薄弱，导致人工智能在商用和军用中失败，迎来了人工智能的第一次寒冬，80年代专家系统的诞生迎来了人工智能的第二个春天，知识库系统和知识工程都得到了普及，紧接着日本推进了第五代计算机项目，各国之间相继开展计算机科技竞赛。随着专家系统的不断发展，数据处理的复杂度快速提升，基于知识库和推理机的专家系统逐渐暴露出缺点，扩展维护工作开展困难，鲁棒性薄弱的缺点导致人工智能再次陷入低谷，随着神经网络的普及，算力的增长，在90 年代，国际象棋冠军卡斯帕罗夫与"深蓝" 计算机决战，"深蓝"获胜，公众再次把注意力放在了人工智能，在2016年，谷歌的AlphaGo赢了韩国棋手李世石，再度引发人工智能热潮。2018年腾讯人工智能实验室推出的“绝艺”在腾讯世界人工智能围棋大赛决赛中以7：0的比分战胜星战围棋\cite{人工智能产业形势分析课题组20182018}，这些都代表了特定时期人工智能发展的技术水平。

人工智能的实现算法在现代被称为机器学习，根据机器学习的应用数据和模型任务，可以分成监督学习，无监督学习和强化学习。本文研究的深度强化学习是在强化学习的框架下引入了深度学习作为函数拟合的工具，下面分别介绍传统的强化学习和深度学习概念。

深度学习是机器学习的重要组成部分，深度学习是利用深层的神经网络进行函数的拟合工作用来建立更加复杂的模型，从而对数据有更深刻全面的理解。深度学习网络把原始数据通过简单的非线性函数进行映射，转换为高层次抽象的表达，通过加大神经网络的深度进行多次非线性映射堆叠，可以表示出非常复杂的模型。通过在学习过程中引入紧缩性和鲁棒性的约束，网络后端的特征输出部分不仅可以强化对输入数据的区分和预测能力，同时可以避免外界噪声和其他不相关因素的干扰，因此在浅层网络无法全面客观表示数据特征，学习数据分布的时候可以采用深度学习网络进行统一的特征学习框架的搭建。深度学习各层的输入特征不是根据具体任务人为处理的，而是利用一种通用的映射和优化的过程从原始数据中学习得到。特征提取工作和分类预测任务被深度学习框架进行统一整合，避免了繁琐的人类手工设计提取特征的过程，实现了端到端的自动学习。准确的说深度学习首先利用无监督学习对每一层的网络节点进行预训练学习模型，并将当前层的输出数据作为输入数据传入给下一层，最后一层采用监督学习方式，根据网络的最后一层输出结果进行前向传播进行网络参数的整体微调优化。经过若干次的参数迭代可以得到相对稳定的网络结构。
伴随着AlphaGo的诞生，强化学习(Reinforcement Learning)作为其核心算法逐渐成为机器学习研究的另一大热点。从某种意义上讲，强化学习是人工智能的未来。有学者认为，智能系统必须能够在不接受持续监督的情况下自主学习，而强化学习正是其中的最佳代表。一个AI必须能够自己判断对错，只有这样才能扩展到大量的知识和一般技能。学习一个好的，而非新的表征对于解决大多数现实世界中的问题来说具有至关重要的作用。这些表征通常不需要被显式地进行学习，这种学习可以通过内部奖励机制来进行引导，强化学习就是这样一种学习方式。机器学习是个跨学科的研究领域，而强化学习则是其中跨学科性质非常显著的一个分支。强化学习理论的发展受到生理学、神经科学和最优控制等领域的启发，现在依旧在很多相关领域被研究。在控制理论、机器人学、运筹学、经济学等领域内部，依旧有很多的学者投身RL的研究，类似的概念或算法往往在不同的领域被重新发明，起了不同的名字。

追随强化学习的发展历史，从1956年Bellman提出动态规划方法，到1977年提出的自适应动态规划算法以及1988年的时间差分算法，1922年Q-learning算法首次被提出，Q-learning也是目前应用最广的算法之一。后来又衍生出很多改进的强化学习算法，包括Saras算法，置信上限树算法，确定性策略梯度算法，等等。强化学习被广泛应用在仿真模拟\cite{傅启明2014一种基于线性函数逼近的离策略}、工业制造\cite{高阳2007平均奖赏强化学习算法研究}、优化与调度\cite{魏英姿2005一种基于强化学习的作业车间动态调度方法}，机器人控制\cite{Ipek2008Self}、游戏博弈\cite{Tesauro1944TD}等领域。

强化学习的核心思想就是以最大化累积奖励为目标，获取完成目标的最优决策方案，因此相比于其他机器学习算法，强化学习更加注重学习解决问题的策略。例如，微软利用强化学习为MSN上的新闻故事选标题，点击该标题的访问者越多，得到的奖励越多，这意味着一个良好的强化学习系统能够做到对明确的优化目标进行奖励最大化。Alphago围棋，也是强化学习的又一个重要的应用场景，在这里最核心的就是如何建立建立情景\raisebox{0.3mm}{----}动作映射(map situations to actions)。智能体并没有明确告知当前最优的动作选择，而是希望通过不断的尝试进行若干个相互关联的动作之后得到最大的总奖励和。当前的行动不仅会影响到即时收益和当前的状态空间的转移同样也会影响所有后续的奖励之和。因此，试错和延迟收益是强化学习两个重要特征。
\subsection{课题研究意义}
早期的强化学习需要人为干预特征的提取，然而人类的认知并不一定是全面完备的，人工提取特征进行标注同时需要消耗大量的资源。受深度学习提取特征的启发，通过深度学习网络代替手工设计的功能或领域启发式算法进行前期的特征提取工作，智能体直接从原始输入构建和获取自己的知识库，增强了强化学习的特征学习能力。这种利用深度学习工具进行特征构建和函数拟合的强化学习算法称为深度强化学习。
深度强化学习结合了深度学习感知能力强和传统强化学习优秀的决策能力的优点，通过端对端的学习方式实现了组合策略的能力，做到了对原始数据从输入到输出的直接控制。其算法的整体框架如图\ref{fig:DRL}所示。
\begin{figure}[h]
	\centering
	\includegraphics[width=\hsize]{example/DRL.png}
	\bicaption[深度强化学习框架]
	{深度强化学习框架}
	{The fremework of deep reinforcement learning}
	\label{DRL}
\end{figure}

近年来深度强化学习在以惊人的速度发展，各个团队相继在探索强化学习在不同领域的新方法和新应用。其进步的速度是有目共睹的，在不到两年的时间里，深度强化学习衍生了很多新算法，如：深度Q网络(Deep Q Network,DQN)\cite{Roderick2017Implementing}, AlphaGo\cite{Silver2016Mastering},以及可微分神经计算机\cite{Graves2016Hybrid},我们也见证了深度强化学习在很多应用上的进展，注意力和机制也都得到了很大的关注。在应用上，决斗网络(dueling network)架构\cite{Wang2015Dueling},ACL上的口语对话系统\cite{Su2016On},EMNLP上的信息提取\cite{Narasimhan2016Improving},以及 NIPS上的价值迭代网络(value iteration networks)\cite{Tamar2017Value}。激动人心的成就比比皆是：异步方法\cite{Mnih2016Asynchronous},用于机器翻译的双学习（dual learning）\cite{Xia2016Dual},生成对抗式模仿学习\cite{Ho2016Generative},无监督强化和辅助学习\cite{Jaderberg2016Reinforcement},神经架构设计\cite{Pham2018Efficient}等等。新的学习机制也在出现，例如把强化学习和无监督，半监督，迁移学习结合提升学习的质量和速度。新的强化学习算法还在不断涌现，学习模型会带来稳定性，收敛性，数据效率，可扩展性，速度，简洁性，可解释性，稳健性和安全性的问题，奖励反馈机制的建立也是很重要的一环，奖励可能来自认知科学领域，涉及物理学，因果模型，组合性学习等各个领域的知识。。强化学习作为一种更为通用的学习和决策范式，将会给深度学习、机器学习和广义上的人工智能带来深远的影响。

\section{深度强化学习国内外研究现状}
在高级人工智能领域，感知和决策都是衡量智能的重要指标。尽管强化学习在理论和算法层面已经取得了一系列的成就，然而通过直接感知高维数据的输入(如图像，语音等)去控制智能体，对强化学习来说仍是长期的任务和挑战。传统的强化学习算法大部分的成功应用方案依赖于人工特征的提取，学习结果的好坏严重依赖于特征选取的质量。深度学习具有较强的特征提取能力但是不具备决策能力，强化学习具有很强的决策能力，对感知问题束手无策，因此，将两者结合起来，优势互补，为复杂系统的感知与决策问题提供了有效的解决思路。

\subsection{深度强化学习国外研究成果}
深度强化学习研究的初期思路就说利用神经网络完成数据的降维任务，将较高维度的特征经过神经网络变换到较低维度，降低强化学习处理的难度。典型的应用包括利用千层神经网络提取视觉信号的输入，控制机器人推箱子\cite{Shibata2003Acquisition},将强化学习和深层自动编码机结合，利用“视觉动作学习”完善智能体的感知能力\cite{Lange2010Deep},将深度置信网络代替传统的值函数逼近器，用于车牌字符分割识别\cite{Abtahi2015A}，深度拟合Q学习\cite{Lange2012Autonomousd}利用视觉输入信号结合深度强化学习框架用于车辆控制的问题，该算法的输入数据为跑道和车辆图像，通过深度学习网络提取特征利用Q学习最后输出合适的控制策略。同时期实现视频赛车自动驾驶策略规划的还有利用神经演化方法(neural evolution, NE)和强化学习进行结合\cite{Kumar2014Understanding}。

受上述前期工作的启发，卷积神经网络在图像处理领域的优秀表现使很多学者开始研究将卷积神经网络和强化学习算法进行结合处理图像数据的感知决策任务。深度Q网络(deep Q network, DQN)\cite{Mnih2013Playing}在2013年被深智团队首次提出，该算法的思想是将CNN网络和Q学习进行结合，利用经验回放技术优化网络。DQN网络是深度强化学习领域的突破，其输入数据是原始时间顺序上连续4帧的游戏画面，经过多层卷积神经网络和全连接网络，输出最后的状态动作Q函数，首次实现了端到端的学习控制。DQN网络的出现引发了深度强化学习的热潮，众多学者提出了很多改进方案，Schaul提出了带优先级经验回放的深度Q网络\cite{Schaul2015Prioritized}，通过对经验进行先后顺序排序，增加相对重要历史数据的回放比例来提高学习效果，弥补了原来DQN网络经验回访技术没有考虑历史数据重要程度的缺点。为了改进Q网络训练过程缓慢的问题，Nair等提出了深度Q网络的大规模分布式架构Gorila\cite{Goeringer2013Massively}，该方法极大的提高了深度Q网络的学习速率。Guo等首次将蒙特卡洛搜索树和原始深度Q网络结合\cite{Guo2014Deep}，用于实时处理Atari游戏，效果好于原始深度Q网络。双重深度Q网络(double-DQN)\cite{DQN}将两个Q学习网络运用到深度Q网络中，得到更加稳定的学习策略，从而解决了原始Q网络中对由于固有估计误差造成的过高估计动作值函数的的问题。受优势学习(advantage learning)的启发，Wang等提出了一种适用于免模型学习(model free) 的竞争架构 (dueling architecture),实验结果表明，利用竞争架构的深度Q网络可以得到相对更加稳定的评估策略。解决探索和策略的问题一直是深度强化学习长期关注的核心，高效的探索策略用于解决复杂环境中的深度强化学习问题具有深远意义。Osband等首次提出一种引导式(boot-strapped)深度Q网络\cite{Osband2016Deep}，引入了随机值函数提升了探索的速率和效率。为了加快强化学习的训练速率，Mnih等改进了异步深度强化学习方法\cite{Mnih2016Asynchronous}。
\subsection{深度强化学习国内研究成果}
中国的技术公司并不示弱，其实，他们做得更加激进，用深度强化学习做直接跟钱挂钩的业务落地。阿里、腾讯、百度、滴滴和天壤等国内团队将深度强化学习应用到搜索、推荐、营销、派单和路径规划等实际问题的决策任务中。并且有公司宣称自己使用了深度强化学习在无人驾驶产品中。
2018年阿里发行了第一本人工智能强化学习电子书\raisebox{0.3mm}{----}《从虚拟世界走进现实应用——强化学习在阿里的技术演进与业务创新》， 平台作为信息的载体，需要在与消费者的互动过程中，根据对消费者（环境）的理解，及时调整 提供信息（商品、客服机器的回答、路径选择等）的策略，从最化过程累 积收益（消费者在平台上的使体验）。基于监督学习式的信息提供段，缺少有效的探索能，系统倾向于给消费者推送曾经发过为的信息单元（商品、店铺或问题答案）。强化学习作为种有效的基于户与系统交互过程建模和最化过程累积收益的学习法，在某些具体的业务场景中进了很好的实践并得到大规模应。书中结合搜索场景，推荐场景，智能客服，广告系统等不同业务场景进行了详细的介绍。

从2016年起，腾讯AI Lab开始研发围棋人工智能程序，于2017年分别在“AI龙星战”和“UEC杯”等世界计算机围棋大赛上斩获冠军，2018年加强版的围棋智能程序“绝艺”问世，首次在让子棋中战胜顶级职业棋手柯洁九段和连笑九段。让子棋是人类通过人工智能不断探索围棋边界的典型范例。

百度也相继提出把强化学习应用在信息流质量的提升中，利用强化学习识别移除标题党，并在凤巢广告系统里部署了强化学习系统，充分利用了强化学习不需要数据标注，可以有效利用更多数据信息，能实现在线学习等优点。同时强化学习的部署也是对工程能力的挑战。在强化学习模型中，广告的优化行为是通过广告投放系统的所带来的奖励(点击率，转化率)，观察广告竞价状态实现的。

深度强化学习通过不断的进步和发展, 在理论与应用层面取得了长足的进步. 表格\ref{tab:1}总结了深度强化学习发展历程中的重要事件。
\begin{table}[!hpb]
	\centering
	\bicaption[深度强化学习研究历程]
	{深度强化学习研究历程}
	{Timeline of deep reinforcement learning research events.}
	\label{tab:1}
	\begin{tabular}{ll}
		年份 & 事件 \\ \midrule
		2013 & Mnih 等提出了深度强化学习的开创性工作DQN在视频游戏领域取得突破 \\
		2014& Guo 等提出DQN与MCTS结合的算法 \\
		2015 &Van 等提出了双重深度Q网络(double-DQN) \\
		2015&Hausknecht等结合LSTM提出了深度递归Q网络(DRQN)\\
		2015&Sorokin等结合注意力网络提出了深度注意力递归Q网络(DARQN)\\
		2016&深智团队在《Nature》上面发表了基于DRL的计算机围棋程序-Alphago\\
		2017&深智团队改进AlphaZero问世\\
		2018&腾讯AI LAb的“绝艺”斩获冠军\\
		 \bottomrule
	\end{tabular}
\end{table}
\subsection{现有研究不足和展望}
尽管随着深度强化学习的不断发展，越来越多的实际问题得到了有效的解决，深度强化学习在理论方面和应用方面仍然存在很多不足之处，如何有效解决这些缺点并拓展强化学习的应用场景将成为以后研究的重点方向。
不足包括：
\begin{enumerate}
	\item 当强化学习智能体的动作具体到生成数据时变成了生成式对抗网络(Generative Adversarial Networks,GAN),现有的生成式对抗网络模型大多数应用于图像视频数据的生成任务上，应用于非图像类数据的场景比较少，同时存在模式崩溃，生成样本数据分布覆盖不足的缺点。
	\item 深度强化学习的另一个主要应用于Atari视频游戏的自主决策，在这里的奖励是通过不断和游戏环境进行交互得到的。在其他应用场景奖励函数的选择和设计显得尤为重要。深度强化学习模型的泛化能力较弱，对于某些场景下表现效果好的模型不一定适用于其他场景。所以模型效果也直接依赖于环境的搭建。
	\item 目前强化学习解决实际问题中最成功的应用是与蒙特卡罗树搜索结合的方法进行一对一智能体两人零和完全信息博弈。多智能体博弈，包括零和，非零和，完全信息，不完全信息博弈的问题都需要进一步研究和扩展。机器学习算法的前提是数据在相同的特征空间具有相同的数据分布，但是现实环境的模型是比较复杂的，这种强制假设限制了深度强化学习的应用范围。可以考虑结合其他模型突破同数据分布的假设。
	\item 深度强化学习奖励反馈机制的建立是又一大难题，在部分应用场景反馈可以是确定性函数，但是大部分的应用场景反馈来自和环境的交互，所以需要把深度强化学习知识和其他学科进行结合，包括心理学，数据科学，计算机科学，神经网络感知科学，等等。
	\item 虽然深度强化学习有广阔的应用前景有待挖掘，但是很多应用者对于其理论层面的研究还不够深入，目前为止深度强化学习相关的文章并没有给出其学习过程收敛性的证明，只能通过是实验结果观察模型的学习效果。对模型的准确认知对于模型的完善和应用是非常关键的。从长远发展来看，理论基础的夯实会利于深度强化学习的良性发展。
\end{enumerate}
\section{主要内容与章节安排}
\subsection{本文的主要工作和创新点}
针对深度强化学习和博弈模型结合的应用现状，本文的创新点如下：
\begin{enumerate}
	\item 具体化强化学习任务，利用GAN生成式对抗网络对数据集进行扩充和预测，并在现有WGAN网络的基础上引入监督信息，完善学习数据分布信息。
	\item 利用self-paly方式不依赖于人类专家数据进行智能体博弈仿真数据生成，根据相对熵自适应调整学习率加快深度学习网络收敛，结合$softmax$和$\varepsilon$贪心策略解决强化学习中探索和利用平衡的问题，并以五子棋为例进行实验效果分析。
	\item 把深度强化学习应用于飞机对战场景，由一对一智能体对抗拓展到多智能体之间的对抗和合作，进行奖励的自动分配和对战策略的学习，设计实验场景并进行实验分析。
\end{enumerate}
\subsection{本文的内容安排}
本文主要研究深度强化学习原理及应用。通过对不同强化学习模型的研究和对比，本文实现了基于GAN生成式对抗网络的数据扩充和预测，利用AC模型的单智能体一对一仿真，和利用DDPG模型的多智能体对抗仿真。
具体的研究内容和章节安排如下：

第一章 绪论。说明了本课题的背景及研究意义，指出存在的技术难点，介绍了目前国内外深度强化学习的研究概况。

第二章主要介绍了传统的强化学习的种类和基本原理，以及结合深度学习模型改进之后的强化学习模型的各种变体原理及相关应用。并结合Alphago以及Alphazero的实现过程剖析背后的主要技术。

第三章针对小样本本缺失数据扩充的具体问题，利用不同的数据集，包括电子设备参数数据的扩充以及不同股票收盘指数数据，经过前期数据处理后进行数据的扩充，由于GAN网络应用场景主要是图像数据，在本章节提出针对数值类数据的改进的引入监督信息的GAN生成式对抗网络对数据进行扩充和预测，利用实验结果分析模型的收敛性以及数据扩充效果。

第四章针对棋盘类游戏零和博弈的问题，提出了类比Alphazero原理的的无先验数据进行训练的Actor-Critic强化学习模型，针对深度学习网络收敛慢的问题引入了基于相对熵的自适应学习率，针对强化学习探索利用平衡的问题引入了噪声系数，以五子棋为实验场景，根据最后人机博弈和智能体自我博弈作战的策略以及训练的损失函数曲线进行分析。改进的算法收敛更快，模型策略更优。

第五章针对飞机作战场景，设计飞机一对一作战，根据结果分析场景迁移的可行性，在此基础上设计二对二多智能体竞争合作结合的应用场景，通过深度强化学习实现根据作战规则和最终奖励的分配实现队间飞机和对内飞机奖励的自动分配，根据人机对弈过程分析策略合理性，证明模型的可行性。

第六章总结全文研究内容，针对本文目前研究的不足对今后的工作提出展望。
