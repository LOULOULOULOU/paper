%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

深度强化学习作为一种新兴算法，解决了传统强化学习手工提取特征难，模型稳定性不高的缺点，受到越来越多研究者的关注。目前深度强化学习已经成功运用到如计算机视觉，语音识别，图像处理等领域，然而利用深度强化学习在小样本回归预测数据扩充上研究成果比较少，训练不稳定生成样本覆盖不均的问题还没有有效的解决方法。在端对端的环境感知和学习上，深度强化学习初有成效，但是在策略方面由于缺乏有效的奖励函数限制了其发展。

本文针对样本缺乏情况下如何利用强化学习模型进行回归数据的扩充，以及在没有专家数据的情况下如何学习一对一智能体的对抗策略和二对二合作竞争结合的对抗策略展开了研究。本文的主要贡献有：

第一，针对数值类数据的特点及GAN生成样本覆盖不均的问题，提出有监督信号的WGAN网络，结合自编码机模型和WGAN模型，利用自编码机对原始数据进行编码，用WGAN进行解码，通过最小化重构误差增加先验信息。经过在多组标准数据集和实测数据集上验证，利用该算法扩充后的数据质量要高于传统算法和其他GAN模型，同时模型的收敛速度更快，得到的模型结果更稳定。

第二，针对棋盘类博弈模型收敛速度慢，搜索空间较大，难以训练的特点，提出了引导式基于交叉熵的自适应学习率的深度强化学习方法，在强化学习的动作选择上，该算法结合了贪心策略和$softmax$策略，进一步改进了强化学习探索和利用难以平衡的问题。在用深度学习拟合策略函数时，使用相对熵调整学习步长，加快了模型的收敛速度，最后以五子棋为实验背景，分析模型自博弈过程和人机博弈过程的状态，证明了算法的有效性。

第三，针对无先验数据的飞机空战问题，提出了以飞机对战为背景用强化学习结合博弈原理产生自博弈数据进行网络训练，结合一对一问题分析模型的可行性后，创新性的把一对一问题拓展到二对二，在既有合作又有竞争的情况下，改变树模型节点的建立方式，实现了队间飞机相互博弈，队内飞机奖励自动分配的过程。最后通过建立飞机对战模型，分析模型策略输出和奖励输出，验证算法可行性。

本文通过三个核心实验，分别从数据扩充，模型训练方式优化，以及二对二竞争合作结合等方面研究了深度强化学习在对抗性策略的应用，通过将深度强化学习思想和博弈理论相结合，可以实现小样本情况下数据分布的学习和没有样本和先验规则情况下一对一智能体和二对二智能体的策略学习。为深度强化学习的应用和研究提供了新的思路和方向。

\end{abstract}

\begin{englishabstract}
Deep reinforcement learning has attracted more and more researchers’ attention because it solves shortcomings of traditional reinforcement learning, which is difficult to extract features manually. At present, reinforcement learning has been successfully applied to different fields such as computer vision, nature language processing, etc. Reinforcement learning is effective at the end-to-end problems, but it has limited its development that the lack of effective reward functions. At the meantime, research on data expansion with regressions are little in the research of deep reinforcement, and the problem of uneven coverage of training instability has not yet been solved.

In view of the above problems, the combination of the game model and the deep reinforcement learning model provides the possibility to help the problems in the absence of training samples to be solved. This paper studies how to use the deep reinforcement learning model to expand the regression data in the absence of samples, and how to learn the one-to-one agent confrontation strategy and the two-two cooperation competition strategy in the absence of expert data. The main contributions of this article are:

Firstly, for the characteristics of numerical data and the problem of uneven coverage of GAN generated samples, a WGAN network with supervised signals is proposed. Combining the self-encoder model and the WGAN model, the original data is encoded by the self-encoder and decoded by WGAN with increasing a priori information by minimizing reconstruction errors. The experiments on multiple sets of standard datasets witness that the data quality of the extended algorithm is higher than that of traditional algorithms and other GAN models, and the convergence speed of the model is faster, and the obtained model results are more stable.

Secondly, a guided deep learning method based on cross-entropy adaptive learning rate is proposed for the slow convergence speed of the chessboard game model. In the action selection of reinforcement learning, the algorithm combined with the greedy strategy and the softmax strategy to solve the problem of intensive learning exploration and utilization is difficult to balance. When using the deep learning to fit the strategy function, the relative entropy is used to adjust the learning step size, which accelerates the convergence speed of the model. Finally, using the Gomoku as the experimental background, the state of the game self-game process and the human-machine game process are analyzed, and the algorithm is proved to be effective.

Thirdly, for the air combat problem of aircraft without prior data, it is proposed to use the reinforcement learning combined with the game principle to generate self-game data for network training in the context of aircraft battle. After combining the feasibility of the one-to-one problem analysis model, the innovative The one-to-one problem is extended to two-to-two. In the case of both cooperation and competition, the establishment of tree model nodes is changed, and the process of mutual game between teams and the automatic distribution of aircraft rewards within the team is realized. Finally, by establishing an aircraft battle model, the model strategy output and reward output are analyzed to verify the feasibility of the algorithm.

Through three core experiments, this paper studies the application of deep reinforcement learning in the confrontation strategy from the aspects of data expansion, model training optimization, and decision-making of two-to-two complex models, by combining deep reinforcement learning ideas with game theory. It can realize the learning of data distribution in the case of small samples and the strategy learning of one-to-one agent and two-pair agent without sample and prior rules. It provides new ideas and directions for the application and research of deep reinforcement learning.

\end{englishabstract}

